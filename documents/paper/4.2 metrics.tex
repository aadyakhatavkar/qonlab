\section{Evaluation Metrics}

Forecast performance is evaluated using point forecast accuracy measures. Let
\begin{equation}
e_{t+1} = y_{t+1} - \hat{y}_{t+1|t}
\end{equation}
denote the one-step-ahead forecast error, where $y_{t+1}$ is the realized value and $\hat{y}_{t+1|t}$ is the forecast formed at time $t$. All metrics are computed over the out-of-sample evaluation period of length $H$ and subsequently averaged across Monte Carlo replications.

\subsection{Point Forecast Metrics}

The primary measure of forecast accuracy is the Root Mean Squared Error (RMSE), defined as
\begin{equation}
\text{RMSE} = \sqrt{ \frac{1}{H} \sum_{t=1}^{H} e_t^2 }.
\end{equation}
RMSE penalizes large forecast errors disproportionately and is therefore sensitive to episodes of heightened volatility or abrupt persistence shifts. It provides an overall measure of predictive precision.

Complementing RMSE, the Mean Absolute Error (MAE) is computed as
\begin{equation}
\text{MAE} = \frac{1}{H} \sum_{t=1}^{H} |e_t|.
\end{equation}
MAE is less sensitive to extreme realizations and is particularly informative under heavy-tailed innovation distributions.

To assess systematic forecast distortion, Bias is defined as
\begin{equation}
\text{Bias} = \frac{1}{H} \sum_{t=1}^{H} e_t.
\end{equation}
Bias measures whether forecasts tend to systematically overpredict or underpredict the realized series. This metric is especially relevant in environments with mean shifts.

In addition, the variance of forecast errors is reported to evaluate dispersion independently of systematic bias. It is calculated as
\begin{equation}
\text{Var}(e) = \frac{1}{H} \sum_{t=1}^{H} (e_t - \bar{e})^2,
\end{equation}
where
\begin{equation}
\bar{e} = \frac{1}{H} \sum_{t=1}^{H} e_t
\end{equation}
denotes the average forecast error. Forecast error variance captures the stability of predictions and helps distinguish improvements arising from reduced volatility of errors versus reductions in bias.

All performance measures are averaged across Monte Carlo replications to obtain stable comparisons across forecasting models and structural break designs.

\\subsection{Bias--Variance Decomposition}

To provide deeper insight into forecast performance differences, we decompose the mean squared error into its constituent components:
\\begin{equation}
\\text{MSE} = \\text{Bias}^2 + \\text{Variance},
\\end{equation}
where $\\text{Bias}^2 = (\\bar{e})^2$ and $\\text{Variance} = \\text{Var}(e)$. This decomposition reveals whether performance improvements arise from bias reduction (systematic correction of forecasts) or variance reduction (increased stability). In structurally unstable environments, large breaks tend to dominate the bias term post-break, while estimation variance dominates in stable periods or with small breaks. By reporting this decomposition alongside RMSE, practitioners can identify which aspects of model design contribute to forecast improvements and tailor method selection accordingly.

\\subsection{Density Forecasting and Log Scores}

Beyond point forecasts, we evaluate the quality of predictive densities using the log predictive score (log score), defined as
\\begin{equation}
\\text{LogScore} = \\frac{1}{H} \\sum_{t=1}^{H} \\log f_t(y_{t+1}),
\\end{equation}
where $f_t(y_{t+1})$ denotes the predictive density evaluated at the realized outcome. Higher (less negative) log scores indicate better density calibration. This metric is particularly relevant for models that provide regime-dependent variance estimates (e.g., Markov-switching specifications) and for applications requiring quantile or tail forecasts. When methods differ substantially in log score but minimally in RMSE, this signals that distributional accuracy improvements do not translate to mean forecast gains, suggesting risk-management rather than operational forecast benefits.

\\subsection{Decision Framework: RMSE vs. Log Score}

The choice between methods optimizing RMSE versus log score depends on the downstream decision problem:
\\begin{itemize}
\\item \\textbf{Operational Forecasting:} Focuses on minimizing point forecast error (RMSE/MAE). Relevant for inventory management, sales forecasting, and production planning where conditional means drive decisions.
\\item \\textbf{Risk Management:} Focuses on accurate tail behavior and distributional properties (log score, quantiles). Relevant for Value-at-Risk calculations, portfolio optimization, and stress testing where density shape matters.
\\item \\textbf{Decision Analysis under Ambiguity:} When both point accuracy and density calibration matter, ensemble averaging of RMSE-optimal and log-score-optimal methods provides robust performance across objectives.
\\end{itemize}

In this study, we evaluate both dimensions and explicitly flag instances where methods conflict. Practitioners should select metrics aligning with their specific application.

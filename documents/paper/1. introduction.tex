\section{Introduction}

The assumption of parameter stability is fundamental to statistical forecasting. In practice, however, the relationships captured by model parameters often change over time. Economic structures, institutions, and policy regimes evolve, and when they do, the underlying data-generating process may shift accordingly. As a result, time series frequently exhibit structural breaks. Because forecasting plays a central role in planning, investment decisions, and policy analysis in both the public and private sectors, understanding the impact of structural instability on predictive performance is of primary importance.

A substantial body of literature examines structural breaks and their implications for inference and estimation. Empirical evidence suggests that financial and macroeconomic relationships are often unstable, and forecasting models that perform well in-sample may fail to deliver comparable improvements out-of-sample once the underlying relationship changes (Stock and Watson, 1996; Clark and McCracken, 2001). Much of the economic research on structural breaks has focused on detection, estimation, and inference in the presence of breaks (e.g., Andrews and Kitagawa; Killick, Fearnhead, and Eckley, 2012). While these contributions are essential, the forecasting problem presents a distinct challenge. In real time, breakpoints are typically unknown, and forecasters must rely on methods that are robust to potential instability.

This raises a natural question: how sensitive are different forecasting models to alternative forms of structural change? In particular, do models that explicitly account for regime shifts outperform simpler adaptive procedures, and under what conditions?

To address this question, this paper employs a Monte Carlo simulation framework to evaluate forecasting accuracy under controlled structural break scenarios. Simulation-based analysis is particularly suitable for this purpose because it allows the researcher to operate in an environment where the true data-generating process is known. By construction, we isolate specific types of instability—mean shifts, changes in autoregressive persistence, and volatility shifts—while holding other features constant. In contrast, empirical time series typically exhibit multiple overlapping sources of instability, making it difficult to identify the precise mechanism driving forecast deterioration.

The main contribution of this paper is a systematic comparison of different forecasting approaches across multiple structural break designs. We consider (i) globally estimated linear time-series models that impose parameter constancy (Global SARIMA), (ii) adaptive rolling-window estimation procedures that re-estimate parameters using only recent observations (Rolling SARIMA), and (iii) regime-switching models that explicitly allow parameters to depend on an unobserved state variable (Markov-switching AR). Forecast performance is primarily evaluated using RMSE, MAE, and bias. In selected designs, we further vary the innovation distribution (Gaussian and Student-$t$ with different degrees of freedom) and, for recurring breaks, the level of regime persistence in order to examine how distributional assumptions and switching dynamics affect predictive stability.

The remainder of the paper is organized as follows. Section 2 reviews the literature on simulation-based forecasting environments, structural breaks, and forecast stability. Section 3 describes the data-generating processes and forecasting design. Section 4 presents the Monte Carlo results for single and recurring break scenarios and discusses the relative performance of the competing forecasting methods.